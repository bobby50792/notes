---
sidebar_position: 6
---

# Diagonalization

## Eigenvalues and eigenvectors

A linear operator $T$ on a finite-dimensional vector space $V$ is called **diagonalizable** if there is an ordered basis $\beta$ for $V$ such that $[T]_\beta$ is a diagonal matrix.  A square matrix $A$ is called **diagonalizable** if $L_A$ is diagonalizable.



A square matrix $D = (d_{ij})$ is called a **diagonal matrix** if  
$$
\begin{equation*}
d_{ij} = 0 \quad \text{whenever } i \ne j.
\end{equation*}
$$
Equivalently, all entries off the main diagonal are zero.



Let $T$ be a linear operator on a vector space $V$.  A **nonzero vector** $v \in V$ is called an **eigenvector** of $T$ if there exists a scalar $\lambda$ such that $T(v) = \lambda v$.  The scalar $\lambda$ is called the **eigenvalue** corresponding to the eigenvector $v$.

Let $A$ be in $M_{n \times n}(F)$.  A nonzero vector $v \in F^n$ is called an **eigenvector** of $A$ if $v$ is an eigenvector of $L_A$;  that is, if $Av = \lambda v$ for some scalar $\lambda$.  The scalar $\lambda$ is called the **eigenvalue** of $A$ corresponding to the eigenvector $v$.



Let $A \in M_{n \times n}(F)$.  The polynomial $f(t) = \det(A - t I_n)$ is called the **characteristic polynomial** of $A$.



It is easily shown that similar matrices have the same characteristic polynomial.



Let $T$ be a linear operator on an $n$-dimensional vector space $V$ with ordered basis $\beta$.  We define the **characteristic polynomial** $f(t)$ of $T$ to be the characteristic polynomial of  $A = [T]_\beta$. That is,

$$
\begin{equation*}
f(t) = \det(A - t I_n).
\end{equation*}
$$


We often denote the characteristic polynomial of an operator $T$ by $f(t) = \det(T - t I)$.



### Theorems

1. Let $A \in M_{n \times n}(F)$.  Then a scalar $\lambda$ is an eigenvalue of $A$ if and only if

$$
\begin{equation*}
\det(A - \lambda I_n) = 0.
\end{equation*}
$$



2. Let $A \in M_{n \times n}(F)$.

	(a) The characteristic polynomial of $A$ is a polynomial of degree $n$ with leading coefficient $(-1)^n$.

	(b) $A$ has at most $n$ distinct eigenvalues.



3. Let $T$ be a linear operator on a vector space $V$, and let $\lambda$ be an eigenvalue of $T$.  A vector $v \in V$ is an eigenvector of $T$ corresponding to $\lambda$ if and only if  $v \ne 0$ and $v \in N(T - \lambda I)$.



## Diagonalizability



A polynomial $f(t)$ in $\text{P}(F)$ **splits over $F$** if there are scalars  $c, a_1, \ldots, a_n$ (not necessarily distinct) in $F$ such that
$$
\begin{equation*}
f(t) = c(t - a_1)(t - a_2)\cdots(t - a_n).
\end{equation*}
$$
If $f(t)$ is the characteristic polynomial of a linear operator or a matrix over a field $F$, then the statement that $f(t)$ **splits** is understood to mean that it splits over $F$.

Let $\lambda$ be an eigenvalue of a linear operator or matrix with characteristic polynomial $f(t)$.  The **(algebraic) multiplicity** of $\lambda$ is the largest positive integer $ k $ for which

$$
\begin{equation*}
(t - \lambda)^k
\end{equation*}
$$

is a factor of $f(t)$.



Let $T$ be a linear operator on a vector space $V$, and let $\lambda$ be an eigenvalue of $T$.  Define

$$
\begin{equation*}
E_\lambda = \{ x \in V : T(x) = \lambda x \} = N(T - \lambda I_V).
\end{equation*}
$$

The set $E_\lambda$ is called the **eigenspace** of $T$ corresponding to the eigenvalue $\lambda$.  Analogously, we define the eigenspace of a square matrix $A$ to be the eigenspace of $L_A$.



### Theorems:

1. Let $T$ be a linear operator on a vector space $V$, and let  $\lambda_1, \lambda_2, \ldots, \lambda_k$ be distinct eigenvalues of $T$.  If $v_1, v_2, \ldots, v_k$ are eigenvectors of $T$ such that $\lambda_i$ corresponds to $v_i$  $(1 \le i \le k)$, then $\{v_1, v_2, \ldots, v_k\}$ is linearly independent.

	

	Corollay:  Let $T$ be a linear operator on an $n$-dimensional vector space $V$.  If $T$ has $n$ distinct eigenvalues, then $T$ is diagonalizable.



2. The characteristic polynomial of any diagonalizable linear operator splits.



3. Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $\lambda$ be an eigenvalue of $T$ having multiplicity $m$.  Then
	$$
	\begin{equation*}
	1 \le \dim(E_\lambda) \le m.
	\end{equation*}
	$$



4. Let $T$ be a linear operator on a vector space $V$, and let  $\lambda_1, \lambda_2, \ldots, \lambda_k$ be distinct eigenvalues of $T$.  For each $i = 1,2,\ldots,k$, let $S_i$ be a finite linearly independent subset of the eigenspace $E_{\lambda_i}$.  Then
	$$
	\begin{equation*}
	S = S_1 \cup S_2 \cup \cdots \cup S_k
	\end{equation*}
	$$

	is a linearly independent subset of $V$.



5. Let $T$ be a linear operator on a finite-dimensional vector space $V$ such that the characteristic polynomial of $T$ splits.  Let $\lambda_1, \lambda_2, \ldots, \lambda_k$ be the distinct eigenvalues of $T$.  Then:

	(a)  $T$ is diagonalizable **if and only if** the multiplicity of $\lambda_i$ is equal to $\dim(E_{\lambda_i})$ for all $i$.

	(b)  If $T$ is diagonalizable and $\beta_i$ is an ordered basis for $E_{\lambda_i}$ for each $i$,  then

	$$
	\begin{equation*}
	\beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k
	\end{equation*}
	$$

	is an ordered basis for $V$ consisting of eigenvectors of $T$.





### Applications

1. Fast Matrix Exponentiation

If a matrix $A$ is diagonalizable, there exists an invertible matrix $P$ and a diagonal 	matrix $D$ such that

$$
\begin{equation*}
A = P D P^{-1}.
\end{equation*}
$$

Then, for any positive integer $n$,

$$
\begin{equation*}
A^n = P D^n P^{-1},
\end{equation*}
$$

where

$$
\begin{equation*}
D^n =
\begin{pmatrix}
\lambda_1^n & & 0 \\
& \ddots & \\
0 & & \lambda_k^n
\end{pmatrix}
\end{equation*}
$$

if $D = \operatorname{diag}(\lambda_1,\dots,\lambda_k)$.  Thus computing $A^n$ reduces to raising scalars $\lambda_i$ to the $n$-th power.



2. Solving Linear Differential Equations

The system of differential equations is written in matrix form as

$$
\begin{equation*}
x' = A x,
\end{equation*}
$$

where $x(t)$ is the vector of unknown functions and $A$ is the coefficient matrix.

The main idea is to **diagonalize** $A$.  If

$$
\begin{equation*}
A = Q D Q^{-1},
\end{equation*}
$$

then substituting into the system gives

$$
\begin{equation*}
x' = Q D Q^{-1} x.
\end{equation*}
$$

Define the new variable

$$
\begin{equation*}
y(t) = Q^{-1} x(t),
\end{equation*}
$$

which transforms the system into

$$
\begin{equation*}
y' = D y.
\end{equation*}
$$

Since $D$ is diagonal, this gives **three independent scalar differential equations**, which are easy to solve.  The solution to the original system is obtained by transforming back:

$$
\begin{equation*}
x(t) = Q\, y(t).
\end{equation*}
$$
