---
sidebar_position: 7
---

# Inner Product Spaces



We assume that all vector spaces are over the field $F$, where $F$ denotes either
$\mathbb{R}$ or $\mathbb{C}$.



## Inner products and norms

Let $V$ be a vector space over $F$. An **inner product** on $V$ is a function that assigns, to every ordered pair of vectors $x$ and $y$ in $V$, a scalar in $F$, denoted $\langle x, y\rangle$, such that for all $x, y, z \in V$ and all $c \in F$, the following hold:

(a)  $\langle x + z, y\rangle = \langle x, y\rangle + \langle z, y\rangle.$

(b)  $\langle c x, y\rangle = c \langle x, y\rangle.$

(c)  $\begin{equation*}
\overline{\langle x, y\rangle} = \langle y, x\rangle,
\end{equation*}$

where the bar denotes complex conjugation.

(d)  $\langle x, x\rangle > 0 \quad \text{if } x \ne 0.$

Note that (c) reduces to $\langle x, y\rangle = \langle y, x\rangle$ if $F = \mathbb{R}$.



For $x = (a_1, a_2, \ldots, a_n)$ and $y = (b_1, b_2, \ldots, b_n)$ in $F^n$, define

$$
\begin{equation*}
\langle x, y\rangle = \sum_{i=1}^{n} a_i \overline{b_i}.
\end{equation*}
$$

The inner product in this example is called the **standard inner product** on $F^n$.
When $F = \mathbb{R}$ the conjugations are not needed, and in early courses this
standard inner product is usually called the **dot product** and is denoted by
$x \cdot y$ instead of $\langle x, y\rangle$.



Let $V$ be an inner product space. For $x \in V$, we define the **norm** or **length**
of $x$ by

$$
\begin{equation*}
\|x\| = \sqrt{\langle x, x\rangle}.
\end{equation*}
$$


Let $A \in M_{m \times n}(F)$. We define the **conjugate transpose** or **adjoint**  of $A$ to be the $n \times m$ matrix $A^{*}$ such that  
$$
\begin{equation*}
(A^{*})_{ij} = \overline{A_{ji}} \quad \text{for all } i, j.
\end{equation*}
$$
Let $V = M_{n \times n}(F)$, and define

$$
\begin{equation*}
\langle A, B\rangle = \operatorname{tr}(B^{*} A)
\end{equation*}
$$

for $A, B \in V$.

The inner product on $M_{n \times n}(F)$ in this example is called the **Frobenius inner product**.



A vector space $V$ over $F$ endowed with a specific inner product is called
an **inner product space**. If $F = \mathbb{C}$, we call $V$ a **complex inner product space**, whereas if $F = \mathbb{R}$, we call $V$ a **real inner product space**.



Let $V$ be an inner product space. Vectors $x$ and $y$ in $V$ are  **orthogonal (perpendicular)** if $\langle x, y\rangle = 0$.

A subset $S$ of $V$ is **orthogonal** if any two distinct vectors in $S$ are orthogonal.

A vector $x$ in $V$ is a **unit vector** if  
$$
\begin{equation*}
\|x\| = 1.
\end{equation*}
$$

Finally, a subset $S$ of $V$ is **orthonormal** if $S$ is orthogonal and consists entirely of unit vectors.



Note that if $S = \{v_1, v_2, \ldots, v_r\}$ is orthonormal, then  $\langle v_i, v_j\rangle = \delta_{ij}$, where $\delta_{ij}$ denotes the Kronecker delta.

Also, observe that multiplying vectors by nonzero scalars does not affect their orthogonality, and that if $x$ is any nonzero vector, then  
$$
\begin{equation*}
\frac{1}{\|x\|} x
\end{equation*}
$$
is a unit vector. The process of multiplying a nonzero vector by the reciprocal of its length is called **normalizing**.



### Theorems

1. Let $V$ be an inner product space. Then for $x, y, z \in V$ and $c \in F$,  the following statements are true.

	(a)  $\langle x, y + z\rangle = \langle x, y\rangle + \langle x, z\rangle.$

	(b)  $\langle x, cy\rangle = \overline{c}\,\langle x, y\rangle.$

	(c)  $\langle x, 0\rangle = \langle 0, x\rangle = 0.$

	(d)  $\langle x, x\rangle = 0 \quad \text{if and only if} \quad x = 0.$

	(e) If $\langle x, y\rangle = \langle x, z\rangle$ for all $x \in V$, then $y = z$.



2. Let $V$ be an inner product space over $F$. Then for all $x, y \in V$ and $c \in F$,  the following statements are true.

	(a)  $\|cx\| = |c| \cdot \|x\|.$

	(b)  $\|x\| = 0 \quad \text{if and only if} \quad x = 0.$

	In any case, $\|x\| \ge 0$.

	(c) **(Cauchy–Schwarz Inequality)**  
	$$
	\begin{equation*}
	|\langle x, y\rangle| \le \|x\| \cdot \|y\|.
	\end{equation*}
	$$

	(d) **(Triangle Inequality)**  
	$$
	\begin{equation*}
	\|x + y\| \le \|x\| + \|y\|.
	\end{equation*}
	$$



## The gram–schmidt orthogonalization process and orthogonal complements

Let $V$ be an inner product space. A subset of $V$ is an **orthonormal basis** for $V$ if it is an ordered basis that is orthonormal.



Let $S$ be a nonempty subset of an inner product space $V$.  We define $S^{\perp}$ (read “$S$ perp”) to be the set of all vectors in $V$  that are orthogonal to every vector in $S$; that is,

$$
\begin{equation*}
S^{\perp} = \{ x \in V : \langle x, y\rangle = 0 \text{ for all } y \in S \}.
\end{equation*}
$$

The set $S^{\perp}$ is called the **orthogonal complement** of $S$.

It is easily seen that $S^{\perp}$ is a subspace of $V$ for any subset $S$ of $V$.



### Theorems

1. Let $V$ be an inner product space and $S = \{v_1, v_2, \ldots, v_k\}$  be an orthogonal subset of $V$ consisting of nonzero vectors.  If $y \in \operatorname{span}(S)$, then
	$$
	\begin{equation*}
	y = \sum_{i=1}^{k} \frac{\langle y, v_i\rangle}{\|v_i\|^2}\, v_i.
	\end{equation*}
	$$

​	

​	Corollary: 

​	Let $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$  

​	consisting of nonzero vectors. Then $S$ is linearly independent.



2. **(Gram–Schmidt process)** Let $V$ be an inner product space and $S = \{w_1, w_2, \ldots, w_n\}$  be a linearly independent subset of $V$. Define $S' = \{v_1, v_2, \ldots, v_n\}$,  where $v_1 = w_1$ and
	$$
	\begin{equation*}
	v_k = w_k - \sum_{j=1}^{k-1} 
	\frac{\langle w_k, v_j\rangle}{\|v_j\|^2} \, v_j
	\quad \text{for } 2 \le k \le n.
	\end{equation*}
	$$

	Then $S'$ is an orthogonal set of nonzero vectors such that  $\operatorname{span}(S') = \operatorname{span}(S)$.



3. Let $V$ be a nonzero finite-dimensional inner product space.  Then $V$ has an orthonormal basis $\beta$. Furthermore, if  $\beta = \{v_1, v_2, \ldots, v_n\}$ and $x \in V$, then
	$$
	\begin{equation*}
	x = \sum_{i=1}^{n} \langle x, v_i\rangle \, v_i.
	\end{equation*}
	$$



4. Let $V$ be a finite-dimensional inner product space with an orthonormal basis  $\beta = \{v_1, v_2, \ldots, v_n\}$. Let $T$ be a linear operator on $V$, and  let $A = [T]_{\beta}$. Then for any $i$ and $j$,
	$$
	\begin{equation*}
	A_{ij} = \langle T(v_j), v_i \rangle .
	\end{equation*}
	$$



5. Let $W$ be a finite-dimensional subspace of an inner product space $V$,  and let $y \in V$. Then there exist unique vectors $u \in W$ and $z \in W^{\perp}$  such that $y = u + z$. Furthermore, if $\{v_1, v_2, \ldots, v_k\}$ is an  orthonormal basis for $W$, then
	$$
	\begin{equation*}
	u = \sum_{i=1}^{k} \langle y, v_i\rangle \, v_i.
	\end{equation*}
	$$

  	

​	Corollary: In the notation of Theorem 6.6, the vector $u$ is the unique vector in $W$  

​	that is “closest” to $y$; that is, for any $x \in W$,

$$
\begin{equation*}
\|y - x\| \ge \|y - u\|,
\end{equation*}
$$

​	and this inequality is an equality if and only if $x = u$. 

​	The vector $u$ in the corollary is called the **orthogonal projection** of $y$ on $W$. 



6. Suppose that $S = \{v_1, v_2, \ldots, v_k\}$ is an orthonormal set  in an $n$-dimensional inner product space $V$. Then

	(a)  $S$ can be extended to an orthonormal basis $\{v_1, v_2, \ldots, v_k, v_{k+1}, \ldots, v_n\}$ for $V$.

	(b)  If $W = \operatorname{span}(S)$, then  $S_1 = \{v_{k+1}, v_{k+2}, \ldots, v_n\}$ is an orthonormal basis  for $W^{\perp}$ (using the preceding notation).

	(c)  If $W$ is any subspace of $V$, then  
	$$
	\begin{equation*}
	\dim(V) = \dim(W) + \dim(W^{\perp}).
	\end{equation*}
	$$



## The adjoint of a linear operator

**Lemma.**. Let $V$ be a finite-dimensional inner product space over $\mathbb{F}$, and let 
$g : V \to \mathbb{F}$ be a linear transformation.  Then there exists a unique vector $y \in V$ such that
$$
\begin{equation*}
g(x) = \langle x, y \rangle \quad \text{for all } x \in V.
\end{equation*}
$$


**Proof.**  Let $\beta = \{v_1, v_2, \dots, v_n\}$ be an orthonormal basis for $V$, and let
$$
\begin{equation*}
y = \sum_{i=1}^{n} \overline{g(v_i)}\, v_i.
\end{equation*}
$$


**Theorem.**  Let $V$ be a finite-dimensional inner product space, and let  $T$ be a linear operator on $V$.  Then there exists a unique function $T^* : V \to V$ such that
$$
\begin{equation*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle 
\quad \text{for all } x, y \in V.
\end{equation*}
$$
Furthermore, $T^*$ is linear. The linear operator $T^*$  is called the **adjoint** of the operator $T$.  



### Theorems

1. Let $V$ be a finite-dimensional inner product space, and let  $\beta$ be an orthonormal basis for $V$.  If $T$ is a linear operator on $V$, then
	$$
	\begin{equation*}
	[T^*]_{\beta} = [T]_{\beta}^*.
	\end{equation*}
	$$



2. Let $V$ be an inner product space, and let $T$ and $U$ be linear operators on $V$.
	Then:

	(a) $(T + U)^* = T^* + U^*$;

	(b) $(cT)^* = \overline{c}\, T^*$ for any $c \in \mathbb{F}$;

	(c) $(TU)^* = U^* T^*$;

	(d) $T^{**} = T$;

	(e) $I^* = I$.



3. Let $A$ and $B$ be $n \times n$ matrices. Then:

	(a) $(A + B)^* = A^* + B^*$;

	(b) $(cA)^* = \overline{c}\, A^*$ for all $c \in \mathbb{F}$;

	(c) $(AB)^* = B^* A^*$;

	(d) $A^{**} = A$;

	(e) $I^* = I$.



### Applications

1. **Least squares problem**

	The problem reduces to finding a vector $x_0 \in \mathbb{F}^n$ such that
	$$
	\begin{equation*}
	\|y - A x_0\| \le \|y - A x\|
	\quad \text{for all } x \in \mathbb{F}^n.
	\end{equation*}
	$$

	That is, \(Ax_0\) is the **orthogonal projection** of \(y\) onto the column space of \(A\).



​	The problem is reduced to finding the constants $c$ and $d$ that minimize $E$.
​	(For this reason the line $y = ct + d$ is called the least squares line.)
​	If we let
$$
\begin{equation*}
A=
\begin{pmatrix}
t_1 & 1\\
t_2 & 1\\
\vdots & \vdots\\
t_m & 1
\end{pmatrix},
\qquad
x=
\begin{pmatrix}
c\\
d
\end{pmatrix},
\qquad
y=
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{pmatrix},
\end{equation*}
$$
​	then it follows that
$$
\begin{equation*}
E=\|y-Ax\|^2.
\end{equation*}
$$


​	**Lemma 1.**  Let $A \in M_{m \times n}(F)$, $x \in F^n$, and $y \in F^m$. Then
$$
\begin{equation*}
\langle Ax, y \rangle_m = \langle x, A^* y \rangle_n.
\end{equation*}
$$


​	**Lemma 2.**  Let $A \in M_{m \times n}(F)$. Then
$$
\begin{equation*}
\operatorname{rank}(A^*A) = \operatorname{rank}(A).
\end{equation*}
$$


​	**Theorem.**  Let $A \in M_{m \times n}(F)$ and $y \in F^m$. Then there exists
​	$x_0 \in F^n$ such that
$$
\begin{equation*}
(A^*A)x_0 = A^*y
\end{equation*}
$$
​	and
$$
\begin{equation*}
\|Ax_0 - y\| \le \|Ax - y\|
\quad \text{for all } x \in F^n.
\end{equation*}
$$

​	Furthermore, if $\operatorname{rank}(A) = n$, then
$$
\begin{equation*}
x_0 = (A^*A)^{-1}A^*y.
\end{equation*}
$$


2. **Minimal Solutions to Systems of Linear Equations**

	Even when a system of linear equations $Ax = b$ is consistent, there may be no unique solution.  In such cases, it may be desirable to find a solution of **minimal norm**.

	A solution $s$ to $Ax = b$ is called a **minimal solution** if
	$$
	\begin{equation*}
	\|s\| \le \|u\|
	\end{equation*}
	$$
	for all other solutions $u$.

	The next theorem assures that every consistent system of linear equations has a **unique minimal solution** and provides a method for computing it.

	

	**Theorem.**  Let $A \in M_{m \times n}(F)$ and $b \in F^m$. Suppose that $Ax = b$ is consistent.  Then the following statements are true.

	(a) There exists exactly one minimal solution $s$ of $Ax = b$, and
	$$
	\begin{equation*}
	s \in R(L_{A^*}).
	\end{equation*}
	$$

	(b) The vector $s$ is the only solution to $Ax = b$ that lies in $R(L_{A^*})$; that is,  if $u$ satisfies
	$$
	\begin{equation*}
	(AA^*)u = b,
	\end{equation*}
	$$
	then
	$$
	\begin{equation*}
	s = A^*u.
	\end{equation*}
	$$



## Normal and self-adjoint operators

