---
sidebar_position: 7
---

# Inner Product Spaces



We assume that all vector spaces are over the field $F$, where $F$ denotes either
$\mathbb{R}$ or $\mathbb{C}$.



## Inner products and norms

Let $V$ be a vector space over $F$. An **inner product** on $V$ is a function that assigns, to every ordered pair of vectors $x$ and $y$ in $V$, a scalar in $F$, denoted $\langle x, y\rangle$, such that for all $x, y, z \in V$ and all $c \in F$, the following hold:

(a)  $\langle x + z, y\rangle = \langle x, y\rangle + \langle z, y\rangle.$

(b)  $\langle c x, y\rangle = c \langle x, y\rangle.$

(c)  $\begin{equation*}
\overline{\langle x, y\rangle} = \langle y, x\rangle,
\end{equation*}$

where the bar denotes complex conjugation.

(d)  $\langle x, x\rangle > 0 \quad \text{if } x \ne 0.$

Note that (c) reduces to $\langle x, y\rangle = \langle y, x\rangle$ if $F = \mathbb{R}$.



For $x = (a_1, a_2, \ldots, a_n)$ and $y = (b_1, b_2, \ldots, b_n)$ in $F^n$, define

$$
\begin{equation*}
\langle x, y\rangle = \sum_{i=1}^{n} a_i \overline{b_i}.
\end{equation*}
$$

The inner product in this example is called the **standard inner product** on $F^n$.
When $F = \mathbb{R}$ the conjugations are not needed, and in early courses this
standard inner product is usually called the **dot product** and is denoted by
$x \cdot y$ instead of $\langle x, y\rangle$.



Let $V$ be an inner product space. For $x \in V$, we define the **norm** or **length**
of $x$ by

$$
\begin{equation*}
\|x\| = \sqrt{\langle x, x\rangle}.
\end{equation*}
$$


Let $A \in M_{m \times n}(F)$. We define the **conjugate transpose** or **adjoint**  of $A$ to be the $n \times m$ matrix $A^{*}$ such that  
$$
\begin{equation*}
(A^{*})_{ij} = \overline{A_{ji}} \quad \text{for all } i, j.
\end{equation*}
$$
Let $V = M_{n \times n}(F)$, and define

$$
\begin{equation*}
\langle A, B\rangle = \operatorname{tr}(B^{*} A)
\end{equation*}
$$

for $A, B \in V$.

The inner product on $M_{n \times n}(F)$ in this example is called the **Frobenius inner product**.



A vector space $V$ over $F$ endowed with a specific inner product is called
an **inner product space**. If $F = \mathbb{C}$, we call $V$ a **complex inner product space**, whereas if $F = \mathbb{R}$, we call $V$ a **real inner product space**.



Let $V$ be an inner product space. Vectors $x$ and $y$ in $V$ are  **orthogonal (perpendicular)** if $\langle x, y\rangle = 0$.

A subset $S$ of $V$ is **orthogonal** if any two distinct vectors in $S$ are orthogonal.

A vector $x$ in $V$ is a **unit vector** if  
$$
\begin{equation*}
\|x\| = 1.
\end{equation*}
$$

Finally, a subset $S$ of $V$ is **orthonormal** if $S$ is orthogonal and consists entirely of unit vectors.



Note that if $S = \{v_1, v_2, \ldots, v_r\}$ is orthonormal, then  $\langle v_i, v_j\rangle = \delta_{ij}$, where $\delta_{ij}$ denotes the Kronecker delta.

Also, observe that multiplying vectors by nonzero scalars does not affect their orthogonality, and that if $x$ is any nonzero vector, then  
$$
\begin{equation*}
\frac{1}{\|x\|} x
\end{equation*}
$$
is a unit vector. The process of multiplying a nonzero vector by the reciprocal of its length is called **normalizing**.



### Theorems

1. Let $V$ be an inner product space. Then for $x, y, z \in V$ and $c \in F$,  the following statements are true.

	(a)  $\langle x, y + z\rangle = \langle x, y\rangle + \langle x, z\rangle.$

	(b)  $\langle x, cy\rangle = \overline{c}\,\langle x, y\rangle.$

	(c)  $\langle x, 0\rangle = \langle 0, x\rangle = 0.$

	(d)  $\langle x, x\rangle = 0 \quad \text{if and only if} \quad x = 0.$

	(e) If $\langle x, y\rangle = \langle x, z\rangle$ for all $x \in V$, then $y = z$.



2. Let $V$ be an inner product space over $F$. Then for all $x, y \in V$ and $c \in F$,  the following statements are true.

	(a)  $\|cx\| = |c| \cdot \|x\|.$

	(b)  $\|x\| = 0 \quad \text{if and only if} \quad x = 0.$

	In any case, $\|x\| \ge 0$.

	(c) **(Cauchy–Schwarz Inequality)**  
	$$
	\begin{equation*}
	|\langle x, y\rangle| \le \|x\| \cdot \|y\|.
	\end{equation*}
	$$

	(d) **(Triangle Inequality)**  
	$$
	\begin{equation*}
	\|x + y\| \le \|x\| + \|y\|.
	\end{equation*}
	$$



## The gram–schmidt orthogonalization process and orthogonal complements

Let $V$ be an inner product space. A subset of $V$ is an **orthonormal basis** for $V$ if it is an ordered basis that is orthonormal.



Let $S$ be a nonempty subset of an inner product space $V$.  We define $S^{\perp}$ (read “$S$ perp”) to be the set of all vectors in $V$  that are orthogonal to every vector in $S$; that is,

$$
\begin{equation*}
S^{\perp} = \{ x \in V : \langle x, y\rangle = 0 \text{ for all } y \in S \}.
\end{equation*}
$$

The set $S^{\perp}$ is called the **orthogonal complement** of $S$.

It is easily seen that $S^{\perp}$ is a subspace of $V$ for any subset $S$ of $V$.



### Theorems

1. Let $V$ be an inner product space and $S = \{v_1, v_2, \ldots, v_k\}$  be an orthogonal subset of $V$ consisting of nonzero vectors.  If $y \in \operatorname{span}(S)$, then
	$$
	\begin{equation*}
	y = \sum_{i=1}^{k} \frac{\langle y, v_i\rangle}{\|v_i\|^2}\, v_i.
	\end{equation*}
	$$

​	

​	Corollary: 

​	Let $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$  

​	consisting of nonzero vectors. Then $S$ is linearly independent.



2. **(Gram–Schmidt process)** Let $V$ be an inner product space and $S = \{w_1, w_2, \ldots, w_n\}$  be a linearly independent subset of $V$. Define $S' = \{v_1, v_2, \ldots, v_n\}$,  where $v_1 = w_1$ and
	$$
	\begin{equation*}
	v_k = w_k - \sum_{j=1}^{k-1} 
	\frac{\langle w_k, v_j\rangle}{\|v_j\|^2} \, v_j
	\quad \text{for } 2 \le k \le n.
	\end{equation*}
	$$

	Then $S'$ is an orthogonal set of nonzero vectors such that  $\operatorname{span}(S') = \operatorname{span}(S)$.



3. Let $V$ be a nonzero finite-dimensional inner product space.  Then $V$ has an orthonormal basis $\beta$. Furthermore, if  $\beta = \{v_1, v_2, \ldots, v_n\}$ and $x \in V$, then
	$$
	\begin{equation*}
	x = \sum_{i=1}^{n} \langle x, v_i\rangle \, v_i.
	\end{equation*}
	$$



4. Let $V$ be a finite-dimensional inner product space with an orthonormal basis  $\beta = \{v_1, v_2, \ldots, v_n\}$. Let $T$ be a linear operator on $V$, and  let $A = [T]_{\beta}$. Then for any $i$ and $j$,
	$$
	\begin{equation*}
	A_{ij} = \langle T(v_j), v_i \rangle .
	\end{equation*}
	$$



5. Let $W$ be a finite-dimensional subspace of an inner product space $V$,  and let $y \in V$. Then there exist unique vectors $u \in W$ and $z \in W^{\perp}$  such that $y = u + z$. Furthermore, if $\{v_1, v_2, \ldots, v_k\}$ is an  orthonormal basis for $W$, then
	$$
	\begin{equation*}
	u = \sum_{i=1}^{k} \langle y, v_i\rangle \, v_i.
	\end{equation*}
	$$

  	

​	Corollary: In the notation of Theorem 6.6, the vector $u$ is the unique vector in $W$  

​	that is “closest” to $y$; that is, for any $x \in W$,

$$
\begin{equation*}
\|y - x\| \ge \|y - u\|,
\end{equation*}
$$

​	and this inequality is an equality if and only if $x = u$. 

​	The vector $u$ in the corollary is called the **orthogonal projection** of $y$ on $W$. 



6. Suppose that $S = \{v_1, v_2, \ldots, v_k\}$ is an orthonormal set  in an $n$-dimensional inner product space $V$. Then

	(a)  $S$ can be extended to an orthonormal basis $\{v_1, v_2, \ldots, v_k, v_{k+1}, \ldots, v_n\}$ for $V$.

	(b)  If $W = \operatorname{span}(S)$, then  $S_1 = \{v_{k+1}, v_{k+2}, \ldots, v_n\}$ is an orthonormal basis  for $W^{\perp}$ (using the preceding notation).

	(c)  If $W$ is any subspace of $V$, then  
	$$
	\begin{equation*}
	\dim(V) = \dim(W) + \dim(W^{\perp}).
	\end{equation*}
	$$
