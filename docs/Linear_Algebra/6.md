---
sidebar_position: 7
---

# Inner Product Spaces

We assume that all vector spaces are over the field $F$, where $F$ denotes either
$\mathbb{R}$ or $\mathbb{C}$.



## Inner products and norms

Let $V$ be a vector space over $F$. An **inner product** on $V$ is a function that assigns, to every ordered pair of vectors $x$ and $y$ in $V$, a scalar in $F$, denoted $\langle x, y\rangle$, such that for all $x, y, z \in V$ and all $c \in F$, the following hold:

(a)  $\langle x + z, y\rangle = \langle x, y\rangle + \langle z, y\rangle.$

(b)  $\langle c x, y\rangle = c \langle x, y\rangle.$

(c)  $\begin{equation*}
\overline{\langle x, y\rangle} = \langle y, x\rangle,
\end{equation*}$

where the bar denotes complex conjugation.

(d)  $\langle x, x\rangle > 0 \quad \text{if } x \ne 0.$

Note that (c) reduces to $\langle x, y\rangle = \langle y, x\rangle$ if $F = \mathbb{R}$.



For $x = (a_1, a_2, \ldots, a_n)$ and $y = (b_1, b_2, \ldots, b_n)$ in $F^n$, define

$$
\begin{equation*}
\langle x, y\rangle = \sum_{i=1}^{n} a_i \overline{b_i}.
\end{equation*}
$$

The inner product in this example is called the **standard inner product** on $F^n$.
When $F = \mathbb{R}$ the conjugations are not needed, and in early courses this
standard inner product is usually called the **dot product** and is denoted by
$x \cdot y$ instead of $\langle x, y\rangle$.



Let $V$ be an inner product space. For $x \in V$, we define the **norm** or **length**
of $x$ by

$$
\begin{equation*}
\|x\| = \sqrt{\langle x, x\rangle}.
\end{equation*}
$$


Let $A \in M_{m \times n}(F)$. We define the **conjugate transpose** or **adjoint**  of $A$ to be the $n \times m$ matrix $A^{*}$ such that  
$$
\begin{equation*}
(A^{*})_{ij} = \overline{A_{ji}} \quad \text{for all } i, j.
\end{equation*}
$$
Let $V = M_{n \times n}(F)$, and define

$$
\begin{equation*}
\langle A, B\rangle = \operatorname{tr}(B^{*} A)
\end{equation*}
$$

for $A, B \in V$.

The inner product on $M_{n \times n}(F)$ in this example is called the **Frobenius inner product**.



A vector space $V$ over $F$ endowed with a specific inner product is called
an **inner product space**. If $F = \mathbb{C}$, we call $V$ a **complex inner product space**, whereas if $F = \mathbb{R}$, we call $V$ a **real inner product space**.



Let $V$ be an inner product space. Vectors $x$ and $y$ in $V$ are  **orthogonal (perpendicular)** if $\langle x, y\rangle = 0$.

A subset $S$ of $V$ is **orthogonal** if any two distinct vectors in $S$ are orthogonal.

A vector $x$ in $V$ is a **unit vector** if  
$$
\begin{equation*}
\|x\| = 1.
\end{equation*}
$$

Finally, a subset $S$ of $V$ is **orthonormal** if $S$ is orthogonal and consists entirely of unit vectors.



Note that if $S = \{v_1, v_2, \ldots, v_r\}$ is orthonormal, then  $\langle v_i, v_j\rangle = \delta_{ij}$, where $\delta_{ij}$ denotes the Kronecker delta.

Also, observe that multiplying vectors by nonzero scalars does not affect their orthogonality, and that if $x$ is any nonzero vector, then  
$$
\begin{equation*}
\frac{1}{\|x\|} x
\end{equation*}
$$
is a unit vector. The process of multiplying a nonzero vector by the reciprocal of its length is called **normalizing**.



### Theorems

1. Let $V$ be an inner product space. Then for $x, y, z \in V$ and $c \in F$,  the following statements are true.

	(a)  $\langle x, y + z\rangle = \langle x, y\rangle + \langle x, z\rangle.$

	(b)  $\langle x, cy\rangle = \overline{c}\,\langle x, y\rangle.$

	(c)  $\langle x, 0\rangle = \langle 0, x\rangle = 0.$

	(d)  $\langle x, x\rangle = 0 \quad \text{if and only if} \quad x = 0.$

	(e) If $\langle x, y\rangle = \langle x, z\rangle$ for all $x \in V$, then $y = z$.



2. Let $V$ be an inner product space over $F$. Then for all $x, y \in V$ and $c \in F$,  the following statements are true.

	(a)  $\|cx\| = |c| \cdot \|x\|.$

	(b)  $\|x\| = 0 \quad \text{if and only if} \quad x = 0.$

	In any case, $\|x\| \ge 0$.

	(c) **(Cauchy–Schwarz Inequality)**  
	$$
	\begin{equation*}
	|\langle x, y\rangle| \le \|x\| \cdot \|y\|.
	\end{equation*}
	$$

	(d) **(Triangle Inequality)**  
	$$
	\begin{equation*}
	\|x + y\| \le \|x\| + \|y\|.
	\end{equation*}
	$$



## The gram–schmidt orthogonalization process and orthogonal complements

Let $V$ be an inner product space. A subset of $V$ is an **orthonormal basis** for $V$ if it is an ordered basis that is orthonormal.



Let $S$ be a nonempty subset of an inner product space $V$.  We define $S^{\perp}$ (read “$S$ perp”) to be the set of all vectors in $V$  that are orthogonal to every vector in $S$; that is,

$$
\begin{equation*}
S^{\perp} = \{ x \in V : \langle x, y\rangle = 0 \text{ for all } y \in S \}.
\end{equation*}
$$

The set $S^{\perp}$ is called the **orthogonal complement** of $S$.

It is easily seen that $S^{\perp}$ is a subspace of $V$ for any subset $S$ of $V$.



### Theorems

1. Let $V$ be an inner product space and $S = \{v_1, v_2, \ldots, v_k\}$  be an orthogonal subset of $V$ consisting of nonzero vectors.  If $y \in \operatorname{span}(S)$, then
	$$
	\begin{equation*}
	y = \sum_{i=1}^{k} \frac{\langle y, v_i\rangle}{\|v_i\|^2}\, v_i.
	\end{equation*}
	$$

​	

​	Corollary: 

​	Let $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$  

​	consisting of nonzero vectors. Then $S$ is linearly independent.



2. **(Gram–Schmidt process)** Let $V$ be an inner product space and $S = \{w_1, w_2, \ldots, w_n\}$  be a linearly independent subset of $V$. Define $S' = \{v_1, v_2, \ldots, v_n\}$,  where $v_1 = w_1$ and
	$$
	\begin{equation*}
	v_k = w_k - \sum_{j=1}^{k-1} 
	\frac{\langle w_k, v_j\rangle}{\|v_j\|^2} \, v_j
	\quad \text{for } 2 \le k \le n.
	\end{equation*}
	$$

	Then $S'$ is an orthogonal set of nonzero vectors such that  $\operatorname{span}(S') = \operatorname{span}(S)$.



3. Let $V$ be a nonzero finite-dimensional inner product space.  Then $V$ has an orthonormal basis $\beta$. Furthermore, if  $\beta = \{v_1, v_2, \ldots, v_n\}$ and $x \in V$, then
	$$
	\begin{equation*}
	x = \sum_{i=1}^{n} \langle x, v_i\rangle \, v_i.
	\end{equation*}
	$$



4. Let $V$ be a finite-dimensional inner product space with an orthonormal basis  $\beta = \{v_1, v_2, \ldots, v_n\}$. Let $T$ be a linear operator on $V$, and  let $A = [T]_{\beta}$. Then for any $i$ and $j$,
	$$
	\begin{equation*}
	A_{ij} = \langle T(v_j), v_i \rangle .
	\end{equation*}
	$$



5. Let $W$ be a finite-dimensional subspace of an inner product space $V$,  and let $y \in V$. Then there exist unique vectors $u \in W$ and $z \in W^{\perp}$  such that $y = u + z$. Furthermore, if $\{v_1, v_2, \ldots, v_k\}$ is an  orthonormal basis for $W$, then
	$$
	\begin{equation*}
	u = \sum_{i=1}^{k} \langle y, v_i\rangle \, v_i.
	\end{equation*}
	$$

  	

​	Corollary: In the notation of Theorem 6.6, the vector $u$ is the unique vector in $W$  

​	that is “closest” to $y$; that is, for any $x \in W$,

$$
\begin{equation*}
\|y - x\| \ge \|y - u\|,
\end{equation*}
$$

​	and this inequality is an equality if and only if $x = u$. 

​	The vector $u$ in the corollary is called the **orthogonal projection** of $y$ on $W$. 



6. Suppose that $S = \{v_1, v_2, \ldots, v_k\}$ is an orthonormal set in an $n$-dimensional inner product space $V$. Then

	(a)  $S$ can be extended to an orthonormal basis $\{v_1, v_2, \ldots, v_k, v_{k+1}, \ldots, v_n\}$ for $V$.

	(b)  If $W = \operatorname{span}(S)$, then  $S_1 = \{v_{k+1}, v_{k+2}, \ldots, v_n\}$ is an orthonormal basis for $W^{\perp}$ .

	(c)  If $W$ is any subspace of $V$, then  
	$$
	\begin{equation*}
	\dim(V) = \dim(W) + \dim(W^{\perp}).
	\end{equation*}
	$$



## The adjoint of a linear operator

**Lemma.** Let $V$ be a finite-dimensional inner product space over $\mathbb{F}$, and let 
$g : V \to \mathbb{F}$ be a linear transformation.  Then there exists a unique vector $y \in V$ such that
$$
\begin{equation*}
g(x) = \langle x, y \rangle \quad \text{for all } x \in V.
\end{equation*}
$$


**Proof.**  Let $\beta = \{v_1, v_2, \dots, v_n\}$ be an orthonormal basis for $V$, and let
$$
\begin{equation*}
y = \sum_{i=1}^{n} \overline{g(v_i)}\, v_i.
\end{equation*}
$$


**Theorem.**  Let $V$ be a finite-dimensional inner product space, and let  $T$ be a linear operator on $V$.  Then there exists a unique function $T^* : V \to V$ such that
$$
\begin{equation*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle 
\quad \text{for all } x, y \in V.
\end{equation*}
$$
Furthermore, $T^*$ is linear. The linear operator $T^*$  is called the **adjoint** of the operator $T$.  



### Theorems

1. Let $V$ be a finite-dimensional inner product space, and let  $\beta$ be an orthonormal basis for $V$.  If $T$ is a linear operator on $V$, then
	$$
	\begin{equation*}
	[T^*]_{\beta} = [T]_{\beta}^*.
	\end{equation*}
	$$



2. Let $V$ be an inner product space, and let $T$ and $U$ be linear operators on $V$.
	Then:

	(a) $(T + U)^* = T^* + U^*$;

	(b) $(cT)^* = \overline{c}\, T^*$ for any $c \in \mathbb{F}$;

	(c) $(TU)^* = U^* T^*$;

	(d) $T^{**} = T$;

	(e) $I^* = I$.



3. Let $A$ and $B$ be $n \times n$ matrices. Then:

	(a) $(A + B)^* = A^* + B^*$;

	(b) $(cA)^* = \overline{c}\, A^*$ for all $c \in \mathbb{F}$;

	(c) $(AB)^* = B^* A^*$;

	(d) $A^{**} = A$;

	(e) $I^* = I$.



### Applications

1. **Least squares problem**

	The problem reduces to finding a vector $x_0 \in \mathbb{F}^n$ such that
	$$
	\begin{equation*}
	\|y - A x_0\| \le \|y - A x\|
	\quad \text{for all } x \in \mathbb{F}^n.
	\end{equation*}
	$$

	That is, \(Ax_0\) is the **orthogonal projection** of \(y\) onto the column space of \(A\).



​	The problem is reduced to finding the constants $c$ and $d$ that minimize $E$.
​	(For this reason the line $y = ct + d$ is called the least squares line.)
​	If we let
$$
\begin{equation*}
A=
\begin{pmatrix}
t_1 & 1\\
t_2 & 1\\
\vdots & \vdots\\
t_m & 1
\end{pmatrix},
\qquad
x=
\begin{pmatrix}
c\\
d
\end{pmatrix},
\qquad
y=
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{pmatrix},
\end{equation*}
$$
​	then it follows that
$$
\begin{equation*}
E=\|y-Ax\|.
\end{equation*}
$$


​	**Lemma 1.** Let $A \in M_{m \times n}(F)$, $x \in F^n$, and $y \in F^m$. Then
$$
\begin{equation*}
\langle Ax, y \rangle_m = \langle x, A^* y \rangle_n.
\end{equation*}
$$


​	**Lemma 2.** Let $A \in M_{m \times n}(F)$. Then
$$
\begin{equation*}
\operatorname{rank}(A^*A) = \operatorname{rank}(A).
\end{equation*}
$$

​	**Theorem.** Let $A \in M_{m \times n}(F)$ and $y \in F^m$. Then there exists
​	$x_0 \in F^n$ such that
$$
\begin{equation*}
(A^*A)x_0 = A^*y
\end{equation*}
$$
​	and
$$
\begin{equation*}
\|Ax_0 - y\| \le \|Ax - y\|
\quad \text{for all } x \in F^n.
\end{equation*}
$$

​	Furthermore, if $\operatorname{rank}(A) = n$, then
$$
\begin{equation*}
x_0 = (A^*A)^{-1}A^*y.
\end{equation*}
$$


2. **Minimal Solutions to Systems of Linear Equations**

	Even when a system of linear equations $Ax = b$ is consistent, there may be no unique solution.  In such cases, it may be desirable to find a solution of **minimal norm**.

	A solution $s$ to $Ax = b$ is called a **minimal solution** if
	$$
	\begin{equation*}
	\|s\| \le \|u\|
	\end{equation*}
	$$
	for all other solutions $u$.

	The next theorem assures that every consistent system of linear equations has a **unique minimal solution** and provides a method for computing it.

	

	**Theorem.** Let $A \in M_{m \times n}(F)$ and $b \in F^m$. Suppose that $Ax = b$ is consistent.  Then the following statements are true.

	(a) There exists exactly one minimal solution $s$ of $Ax = b$, and
	$$
	\begin{equation*}
	s \in R(L_{A^*}).
	\end{equation*}
	$$

	(b) The vector $s$ is the only solution to $Ax = b$ that lies in $R(L_{A^*})$; that is,  if $u$ satisfies
	$$
	\begin{equation*}
	(AA^*)u = b,
	\end{equation*}
	$$
	then
	$$
	\begin{equation*}
	s = A^*u.
	\end{equation*}
	$$



## Normal and self-adjoint operators

**Lemma.** Let $T$ be a linear operator on a finite-dimensional inner product space $V$. If $T$ has an eigenvector, then so does $T^*$.



**Theorem (Schur).** Let $T$ be a linear operator on a finite-dimensional inner product space $V$.Suppose that the characteristic polynomial of $T$ splits. Then there exists an orthonormal basis $\beta$ for $V$ such that the matrix $[T]_\beta$ is upper triangular.



**Definitions.** Let $V$ be an inner product space, and let $T$ be a linear operator on $V$. We say that $T$ is **normal** if $TT^* = T^*T$. An $n \times n$ real or complex matrix $A$ is **normal** if $AA^* = A^*A$.



**Definitions.** Let $T$ be a linear operator on an inner product space $V$.We say that $T$ is **self-adjoint (Hermitian)** if $T = T^*$. An $n \times n$ real or complex matrix $A$ is **self-adjoint (Hermitian)** if $A = A^*$.



### Theorems

1. Let $V$ be an inner product space, and let $T$ be a normal operator on $V$. Then the following statements are true.

	(a) $\|T(x)\| = \|T^*(x)\|$ for all $x \in V$.

	(b) $T - cI$ is normal for every $c \in F$.

	(c) If $x$ is an eigenvector of $T$, then $x$ is also an eigenvector of $T^*$. In fact, if $T(x) = \lambda x$, then $T^*(x) = \overline{\lambda} x$.

	(d) If $\lambda_1$ and $\lambda_2$ are distinct eigenvalues of $T$ with corresponding eigenvectors $x_1$ and $x_2$, then $x_1$ and $x_2$ are orthogonal.



2. Let $T$ be a linear operator on a finite-dimensional complex inner product space $V$.
	Then $T$ is normal if and only if there exists an orthonormal basis for $V$ consisting of eigenvectors of $T$.



3. Let $T$ be a self-adjoint operator on a finite-dimensional inner product space $V$. Then

	(a) Every eigenvalue of $T$ is real.

	(b) Suppose that $V$ is a real inner product space. Then the characteristic polynomial of $T$ splits.

	

4. Let $T$ be a linear operator on a finite-dimensional real inner product space $V$. Then $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ for $V$ consisting of eigenvectors of $T$.



##  Unitary and Orthogonal Operators and Their Matrices



**Definitions.** Let $T$ be a linear operator on a finite-dimensional inner product space $V$ (over $F$). If $\|T(x)\| = \|x\|$ for all $x \in V$, we call $T$ a **unitary operator** if $F = \mathbb{C}$ and an **orthogonal operator** if $F = \mathbb{R}$.



**Lemma.** Let $U$ be a self-adjoint operator on a finite-dimensional inner product space $V$. If $\langle x, U(x) \rangle = 0$ for all $x \in V$, then $U = T_0$.



**Theorem.** Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Then the following statements are equivalent.

(a) $TT^* = T^*T = I$.

(b) $\langle T(x), T(y) \rangle = \langle x, y \rangle$ for all $x, y \in V$.

(c) If $\beta$ is an orthonormal basis for $V$, then $T(\beta)$ is an orthonormal basis for $V$.

(d) There exists an orthonormal basis $\beta$ for $V$ such that $T(\beta)$ is an orthonormal basis for $V$.

(e) $\|T(x)\| = \|x\|$ for all $x \in V$.



**Definition.** Let $L$ be a one-dimensional subspace of $\mathbb{R}^2$. We may view $L$ as a line in the plane through the origin. A linear operator $T$ on $\mathbb{R}^2$ is called a **reflection** of $\mathbb{R}^2$ about $L$ if
$$
\begin{equation*}
T(x)=x \quad \text{for all } x\in L
\end{equation*}
$$
and
$$
\begin{equation*}
T(x)=-x \quad \text{for all } x\in L^\perp.
\end{equation*}
$$


**Definitions.** A square matrix $A$ is called an **orthogonal matrix** if
$$
\begin{equation*}
A^t A = A A^t = I,
\end{equation*}
$$
and **unitary** if
$$
\begin{equation*}
A^* A = A A^* = I.
\end{equation*}
$$


### Theorems

1. Let $T$ be a linear operator on a finite-dimensional real inner product space $V$. Then $V$ has an orthonormal basis of eigenvectors of $T$ with corresponding eigenvalues of absolute value $1$ if and only if $T$ is both **self-adjoint** and **orthogonal**.



2. Let $T$ be a linear operator on a finite-dimensional complex inner product space $V$. Then $V$ has an orthonormal basis of eigenvectors of $T$ with corresponding eigenvalues of absolute value $1$ if and only if $T$ is **unitary**.

	

3. Let $A$ be a complex $n \times n$ matrix. Then $A$ is **normal** if and only if $A$ is **unitarily equivalent** to a diagonal matrix.



4. Let $A$ be a real $n \times n$ matrix. Then $A$ is **symmetric (self-adjoint)** if and only if $A$ is **orthogonally equivalent** to a real diagonal matrix.



## Orthogonal Projections and the Spectral Theorem



**Definition.** If $V = W_1 \oplus W_2$, then a linear operator $T$ on $V$ is the **projection on $W_1$ along $W_2$** if, whenever
$$
\begin{equation*}
x = x_1 + x_2,
\end{equation*}
$$
with $x_1 \in W_1$ and $x_2 \in W_2$, we have
$$
\begin{equation*}
T(x) = x_1.
\end{equation*}
$$


**Definition.** Let $V$ be an inner product space, and let $T: V \to V$ be a projection. We say that $T$ is an **orthogonal projection** if
$$
\begin{equation*}
R(T)^\perp = N(T)
\end{equation*}
$$
and
$$
\begin{equation*}
N(T)^\perp = R(T).
\end{equation*}
$$


### Theorems

1. Let $V$ be an inner product space, and let $T$ be a linear operator on $V$. Then $T$ is an **orthogonal projection** if and only if $T$ has an adjoint $T^*$ and
	$$
	\begin{equation*}
	T^2 = T = T^*.
	\end{equation*}
	$$



2. **(The Spectral Theorem)** Suppose that $T$ is a linear operator on a finite-dimensional inner product space $V$ over $\mathbb{F}$ with the distinct eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_k$. Assume that $T$ is normal if $\mathbb{F} = \mathbb{C}$ and that $T$ is self-adjoint if $\mathbb{F} = \mathbb{R}$. For each $i$ $(1 \le i \le k)$, let $W_i$ be the eigenspace of $T$ corresponding to the eigenvalue $\lambda_i$, and let $T_i$ be the orthogonal projection of $V$ on $W_i$. Then the following statements are true.

	(a)
	$$
	\begin{equation*}
	V = W_1 \oplus W_2 \oplus \cdots \oplus W_k.
	\end{equation*}
	$$

	(b)  If $W_i'$ denotes the direct sum of the subspaces $W_j$ for $j \ne i$, then
	$$
	\begin{equation*}
	W_i^\perp = W_i'.
	\end{equation*}
	$$

	(c)
	$$
	\begin{equation*}
	T_i T_j = \delta_{ij} T_i \quad \text{for } 1 \le i, j \le k.
	\end{equation*}
	$$

	(d)
	$$
	\begin{equation*}
	I = T_1 + T_2 + \cdots + T_k.
	\end{equation*}
	$$

	(e)
	$$
	\begin{equation*}
	T = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k.
	\end{equation*}
	$$



3. If $\mathbb{F} = \mathbb{C}$, then $T$ is **normal** if and only if
	$$
	\begin{equation*}
	T^* = g(T)
	\end{equation*}
	$$
	for some polynomial $g$.



4. If $\mathbb{F} = \mathbb{C}$, then $T$ is **unitary** if and only if $T$ is normal and
	$$
	\begin{equation*}
	|\lambda| = 1
	\end{equation*}
	$$
	for every eigenvalue $\lambda$ of $T$.



5. If $\mathbb{F} = \mathbb{C}$ and $T$ is normal, then $T$ is **self-adjoint** if and only if every eigenvalue of $T$ is real.



6. Let $T$ be as in the spectral theorem with spectral decomposition
	$$
	\begin{equation*}
	T = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k.
	\end{equation*}
	$$
	Then each $T_j$ is a polynomial in $T$.
